{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Co-segmentation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import iCosegDataset, PASCALVOCCosegDataset\n",
    "from model import SiameseSegNet\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "## Dataset\n",
    "BATCH_SIZE = 2 * 1 # two images at a time for Siamese net\n",
    "INPUT_CHANNELS = 3 # RGB\n",
    "OUTPUT_CHANNELS = 2 # BG + FG channel\n",
    "\n",
    "## Inference\n",
    "CUDA = \"0\"\n",
    "\n",
    "## Output Dir\n",
    "OUTPUT_DIR = \"./experiments\"\n",
    "\n",
    "os.system(f\"rm -r {OUTPUT_DIR}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pmapA, pmapB, masksA, masksB):\n",
    "    intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    for idx in range(BATCH_SIZE//2):\n",
    "        pred_maskA = torch.argmax(pmapA[idx], dim=0).cpu().numpy()\n",
    "        pred_maskB = torch.argmax(pmapB[idx], dim=0).cpu().numpy()\n",
    "\n",
    "        masksA_cpu = masksA[idx].cpu().numpy()\n",
    "        masksB_cpu = masksB[idx].cpu().numpy()\n",
    "\n",
    "        intersection_a += np.sum(pred_maskA & masksA_cpu)\n",
    "        intersection_b += np.sum(pred_maskB & masksB_cpu)\n",
    "\n",
    "        union_a += np.sum(pred_maskA | masksA_cpu)\n",
    "        union_b += np.sum(pred_maskB | masksB_cpu)\n",
    "\n",
    "        precision_a += np.sum(pred_maskA == masksA_cpu)\n",
    "        precision_b += np.sum(pred_maskB == masksB_cpu)\n",
    "\n",
    "    return intersection_a, intersection_b, union_a, union_b, precision_a, precision_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Deep Object Co-segmentation model trained on Pascal VOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINT = \"/home/SharedData/intern_sayan/PASCAL_coseg/\"\n",
    "root_dir = \"/home/SharedData/intern_sayan/iCoseg/\"\n",
    "\n",
    "image_dir = os.path.join(root_dir, \"images\")\n",
    "mask_dir = os.path.join(root_dir, \"ground_truth\")\n",
    "\n",
    "iCoseg_dataset = iCosegDataset(image_dir=image_dir,\n",
    "                               mask_dir=mask_dir)\n",
    "dataloader = DataLoader(iCoseg_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)\n",
    "\n",
    "model = SiameseSegNet(input_channels=INPUT_CHANNELS,\n",
    "                          output_channels=OUTPUT_CHANNELS,\n",
    "                          gpu=CUDA)\n",
    "if DEBUG:\n",
    "    print(model)\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "if CUDA is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    model.load_state_dict(torch.load(os.path.join(LOAD_CHECKPOINT, \"coseg_model_best.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [ Car]  \n",
    "\n",
    "iCoseg class indices = {5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ea716defe34fedb4880e568bc61ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [4.3226869106292725 secs]\n",
      "Precision : [0.013706581614841925]\n",
      "IoU : [0.5131968756333124]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([5,5]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"car_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [ People]\n",
    "iCoseg class indices = {1,4,26,27,28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b56a3944ff249ce8f81c23de9d6cd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [24.818859338760376 secs]\n",
      "Precision : [0.17149745786672813]\n",
      "IoU : [0.4137933753029626]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([1,1]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([4,4]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([26,26]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([26,27]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([27,27]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([27,28]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([28,28]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"people_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"people_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"people_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [ Goose]\n",
    "iCoseg class indices = {10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ae4bca01ce408bbeeed25d73c8269d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [7.996076583862305 secs]\n",
      "Precision : [0.038217502962391695]\n",
      "IoU : [0.3825068982745369]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([10,10]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"goose_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"goose_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"goose_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [ Airplane]  \n",
    "\n",
    "iCoseg class indices = {12,13,14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acdb277df2eb43f28103d0fe3b81a9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [15.882441520690918 secs]\n",
      "Precision : [0.10871250235774435]\n",
      "IoU : [0.2877117845748054]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([12,12]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([12,13]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([13,13]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([13,14]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([14,14]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"airplane_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
