{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Co-segmentation Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import iCosegDataset, PASCALVOCCosegDataset, MSRCDataset, InternetDataset\n",
    "from model import SiameseSegNet\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Debug\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "\n",
    "## Dataset\n",
    "BATCH_SIZE = 2 * 1 # two images at a time for Siamese net\n",
    "INPUT_CHANNELS = 3 # RGB\n",
    "OUTPUT_CHANNELS = 2 # BG + FG channel\n",
    "\n",
    "## Inference\n",
    "CUDA = \"0\"\n",
    "\n",
    "## Output Dir\n",
    "OUTPUT_DIR = \"./experiments\"\n",
    "\n",
    "os.system(f\"rm -r {OUTPUT_DIR}\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics(pmapA, pmapB, masksA, masksB):\n",
    "    intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = 0, 0, 0, 0, 0, 0\n",
    "    \n",
    "    for idx in range(BATCH_SIZE//2):\n",
    "        pred_maskA = torch.argmax(pmapA[idx], dim=0).cpu().numpy()\n",
    "        pred_maskB = torch.argmax(pmapB[idx], dim=0).cpu().numpy()\n",
    "\n",
    "        masksA_cpu = masksA[idx].cpu().numpy()\n",
    "        masksB_cpu = masksB[idx].cpu().numpy()\n",
    "\n",
    "        intersection_a += np.sum(pred_maskA & masksA_cpu)\n",
    "        intersection_b += np.sum(pred_maskB & masksB_cpu)\n",
    "\n",
    "        union_a += np.sum(pred_maskA | masksA_cpu)\n",
    "        union_b += np.sum(pred_maskB | masksB_cpu)\n",
    "\n",
    "        precision_a += np.sum(pred_maskA == masksA_cpu)\n",
    "        precision_b += np.sum(pred_maskB == masksB_cpu)\n",
    "\n",
    "    return intersection_a, intersection_b, union_a, union_b, precision_a, precision_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Deep Object Co-segmentation model trained on Pascal VOC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_CHECKPOINT = \"/home/SharedData/intern_sayan/PASCAL_coseg/\"\n",
    "\n",
    "model = SiameseSegNet(input_channels=INPUT_CHANNELS,\n",
    "                          output_channels=OUTPUT_CHANNELS,\n",
    "                          gpu=CUDA)\n",
    "if DEBUG:\n",
    "    print(model)\n",
    "\n",
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor\n",
    "\n",
    "if CUDA is not None:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = CUDA\n",
    "\n",
    "    model = model.cuda()\n",
    "\n",
    "    FloatTensor = torch.cuda.FloatTensor\n",
    "    LongTensor = torch.cuda.LongTensor\n",
    "\n",
    "if LOAD_CHECKPOINT:\n",
    "    model.load_state_dict(torch.load(os.path.join(LOAD_CHECKPOINT, \"coseg_model_best.pth\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iCoseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/SharedData/intern_sayan/iCoseg/\"\n",
    "\n",
    "image_dir = os.path.join(root_dir, \"images\")\n",
    "mask_dir = os.path.join(root_dir, \"ground_truth\")\n",
    "\n",
    "dataset = iCosegDataset(image_dir=image_dir,\n",
    "                        mask_dir=mask_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [Car]  \n",
    "\n",
    "iCoseg class indices = {5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac398c72e744afa8d9dbb3b9591ec5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [4.99291729927063 secs]\n",
      "Precision : [0.01391393596138167]\n",
      "IoU : [0.5743049860244029]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([5,5]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"car_iCoseg_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_iCoseg_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_iCoseg_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [People]\n",
    "iCoseg class indices = {1,4,26,27,28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b569124d05d14de5bb39bc88dd523281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [25.31199550628662 secs]\n",
      "Precision : [0.1715789093778141]\n",
      "IoU : [0.41228012935962516]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([1,1]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([4,4]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([26,26]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([26,27]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([27,27]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([27,28]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([28,28]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"people_iCoseg_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"people_iCoseg_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"people_iCoseg_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [Goose]\n",
    "iCoseg class indices = {10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87f2b9de94f54a3cbc5282acc97b98f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [8.782491445541382 secs]\n",
      "Precision : [0.03810905147564374]\n",
      "IoU : [0.38239511332035714]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([10,10]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"goose_iCoseg_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"goose_iCoseg_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"goose_iCoseg_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + iCoseg [Airplane]  \n",
    "\n",
    "iCoseg class indices = {12,13,14}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f55aa7c0e064565a2a9251f62299878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [15.402763605117798 secs]\n",
      "Precision : [0.108995289074669]\n",
      "IoU : [0.3660986589923934]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([12,12]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([12,13]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([13,13]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([13,14]))) or \\\n",
    "           torch.equal(batch[\"label\"], torch.from_numpy(np.array([14,14]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"airplane_iCoseg_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_iCoseg_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_iCoseg_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSRC Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/SharedData/intern_sayan/MSRC_processed/\"\n",
    "\n",
    "image_dir = os.path.join(root_dir, \"images\")\n",
    "mask_dir = os.path.join(root_dir, \"GT\")\n",
    "\n",
    "dataset = MSRCDataset(image_dir=image_dir,\n",
    "                      mask_dir=mask_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Car]  \n",
    "\n",
    "MSRC class indices = {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf022ae50fdc4d28b61241462db23232",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [6.370521306991577 secs]\n",
      "Precision : [0.0685278240003084]\n",
      "IoU : [0.3403835207992279]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([1,1]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"car_MSRC_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_MSRC_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_MSRC_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Airplane]  \n",
    "\n",
    "MSRC class indices = {5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc332e79f188402da1552543ad974362",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [7.229506015777588 secs]\n",
      "Precision : [0.061545257933402174]\n",
      "IoU : [0.47602728675268097]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([5,5]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"airplane_MSRC_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_MSRC_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_MSRC_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Bird]  \n",
    "\n",
    "MSRC class indices = {7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbeb03d1b9844800b37b9f982555f78e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [8.12025260925293 secs]\n",
      "Precision : [0.05666641527385803]\n",
      "IoU : [0.43510671340462354]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([7,7]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"bird_MSRC_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Cat]  \n",
    "\n",
    "MSRC class indices = {8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ed51ef703764ad78b23b595511e9f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [6.225841522216797 secs]\n",
      "Precision : [0.05170665175150456]\n",
      "IoU : [0.18139940688218698]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([8,8]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"bird_MSRC_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Cow]  \n",
    "\n",
    "MSRC class indices = {9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cae9a3fe23a4eaaac91c413306d2987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [5.706972599029541 secs]\n",
      "Precision : [0.0424988486550071]\n",
      "IoU : [0.009847018975376588]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([9,9]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"bird_MSRC_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"bird_MSRC_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Internet Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"/home/SharedData/intern_sayan/internet_processed/\"\n",
    "\n",
    "image_dir = os.path.join(root_dir, \"images\", \"Data\")\n",
    "mask_dir = os.path.join(root_dir, \"GT\", \"Data\")\n",
    "\n",
    "dataset = InternetDataset(image_dir=image_dir,\n",
    "                          mask_dir=mask_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + Internet [Airplane]  \n",
    "\n",
    "MSRC class indices = {0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae7bc5e3a2e46b285b385c5250ac034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [14.892923831939697 secs]\n",
      "Precision : [0.45864110946655273]\n",
      "IoU : [0.42364249199214404]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([0,0]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"airplane_Internet_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_Internet_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"airplane_Internet_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VOC + MSRC [Car]  \n",
    "\n",
    "MSRC class indices = {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ee410f775ac4f008dc456ce5390ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Time elapsed: [16.340217113494873 secs]\n",
      "Precision : [0.4201101875305176]\n",
      "IoU : [0.5177999570587426]\n"
     ]
    }
   ],
   "source": [
    "def infer():\n",
    "    model.eval()\n",
    "\n",
    "    intersection, union, precision = 0, 0, 0\n",
    "    t_start = time.time()\n",
    "\n",
    "    for batch_idx, batch in tqdm_notebook(enumerate(dataloader)):\n",
    "        images = batch[\"image\"].type(FloatTensor)\n",
    "        labels = batch[\"label\"].type(LongTensor)\n",
    "        masks  = batch[\"mask\"].type(FloatTensor)\n",
    "        \n",
    "        if torch.equal(batch[\"label\"], torch.from_numpy(np.array([1,1]))):\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            pairwise_images = [(images[2*idx], images[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_labels = [(labels[2*idx], labels[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "            pairwise_masks  = [(masks[2*idx], masks[2*idx+1]) for idx in range(BATCH_SIZE//2)]\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = zip(*pairwise_images)\n",
    "            labelsA, labelsB = zip(*pairwise_labels)\n",
    "            masksA, masksB = zip(*pairwise_masks)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            imagesA, imagesB = torch.stack(imagesA), torch.stack(imagesB)\n",
    "            labelsA, labelsB = torch.stack(labelsA), torch.stack(labelsB)\n",
    "            masksA, masksB = torch.stack(masksA).long(), torch.stack(masksB).long()\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            eq_labels = []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                if torch.equal(labelsA[idx], labelsB[idx]):\n",
    "                    eq_labels.append(torch.ones(1).type(LongTensor))\n",
    "                else:\n",
    "                    eq_labels.append(torch.zeros(1).type(LongTensor))\n",
    "\n",
    "            eq_labels = torch.stack(eq_labels)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            masksA = masksA * eq_labels.unsqueeze(1)\n",
    "            masksB = masksB * eq_labels.unsqueeze(1)\n",
    "\n",
    "            imagesA_v = torch.autograd.Variable(FloatTensor(imagesA))\n",
    "            imagesB_v = torch.autograd.Variable(FloatTensor(imagesB))\n",
    "\n",
    "            pmapA, pmapB, similarity = model(imagesA_v, imagesB_v)\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            res_images, res_masks, gt_masks = [], [], []\n",
    "\n",
    "            for idx in range(BATCH_SIZE//2):\n",
    "                res_images.append(imagesA[idx])\n",
    "                res_images.append(imagesB[idx])\n",
    "\n",
    "                res_masks.append(torch.argmax((pmapA * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "                res_masks.append(torch.argmax((pmapB * similarity.unsqueeze(2).unsqueeze(2))[idx],\n",
    "                                              dim=0).reshape(1, 512, 512))\n",
    "\n",
    "                gt_masks.append(masksA[idx].reshape(1, 512, 512))\n",
    "                gt_masks.append(masksB[idx].reshape(1, 512, 512))\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            images_T = torch.stack(res_images)\n",
    "            masks_T = torch.stack(res_masks)\n",
    "            gt_masks_T = torch.stack(gt_masks)\n",
    "\n",
    "\n",
    "            # metrics - IoU & precision\n",
    "            intersection_a, intersection_b, union_a, union_b, precision_a, precision_b = metrics(pmapA,\n",
    "                                                                                                 pmapB,\n",
    "                                                                                                 masksA,\n",
    "                                                                                                 masksB)\n",
    "\n",
    "            intersection += intersection_a + intersection_b\n",
    "            union += union_a + union_b\n",
    "\n",
    "            precision += (precision_a / (512 * 512)) + (precision_b / (512 * 512))\n",
    "\n",
    "\n",
    "            # pdb.set_trace()\n",
    "\n",
    "            torchvision.utils.save_image(images_T,\n",
    "                                         os.path.join(OUTPUT_DIR,f\"car_Internet_{batch_idx}_images.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_Internet_{batch_idx}_masks.png\"),\n",
    "                                         nrow=2)\n",
    "            torchvision.utils.save_image(gt_masks_T,\n",
    "                                         os.path.join(OUTPUT_DIR, f\"car_Internet_{batch_idx}_gt_masks.png\"),\n",
    "                                         nrow=2)\n",
    "\n",
    "    delta = time.time() - t_start\n",
    "\n",
    "    print(f\"\\nTime elapsed: [{delta} secs]\\nPrecision : [{precision/(len(dataloader) * BATCH_SIZE)}]\\nIoU : [{intersection/union}]\")\n",
    "    \n",
    "    \n",
    "infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset\n",
    "del dataloader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
